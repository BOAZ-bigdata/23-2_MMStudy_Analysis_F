{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. 합성곱 신경망**\n",
    "#### **목차**\n",
    "- 5.1. 합성곱 신경망\n",
    "- 5.2. 합성곱 신경망 맛보기\n",
    "- 5.3. 전이 학습\n",
    "- 5.4. 설명 가능한 CNN\n",
    "- 5.5. 그래프 합성곱 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.1. 합성곱 신경망**\n",
    "##### 5.1.1 합성곱층의 필요성\n",
    "- 이미지 데이터의 경우 데이터의 공간적 구조를 무시하지 않기 위해 등장\n",
    "##### 5.1.2 합성곱 신경망 구조\n",
    "- 합성곱 신경망의 계층\n",
    "    - 입력층\n",
    "    - 합성곱층\n",
    "    - 풀링층\n",
    "        - 특성 맵의 차원을 다운 샘플링하여 연산량을 감소\n",
    "        - 종류\n",
    "            - 최대 풀링\n",
    "            - 평균 풀링\n",
    "    - 완전연결층\n",
    "    - 출력층\n",
    "##### 5.1.3 1D,2D,3D 합성곱\n",
    "- 1D 합성곱\n",
    "    - 필터가 시간을 축으로 좌우로만 이동할 수 있는 합성곱\n",
    "    - 출력 형태: 1D 배열\n",
    "- 3D 입력을 갖는 2D 합성곱\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2. 합성곱 신경망 맛보기**\n",
    "- fashion_mnist 데이터셋을 사용하여 합성곱 신경망을 직접 구현해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\"../chap05/data\",download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\"../chap05/data\",download=True,train=False,transform=transforms.Compose([transforms.ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 데이터 사이즈와 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 60000 images\n",
      "Train Data Image Size: torch.Size([100, 1, 28, 28]) Label Size : torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "from data import load_data_using_mini_batch \n",
    "\n",
    "# count images in train data set\n",
    "print(f\"Train Data: {len(train_dataset)} images\")\n",
    "\n",
    "# put dataset in torch data loader\n",
    "mini_batch_size = 100\n",
    "train_dataloader = load_data_using_mini_batch(train_dataset,mini_batch_size)\n",
    "test_dataloader = load_data_using_mini_batch(test_dataset,mini_batch_size)\n",
    "\n",
    "# check image size\n",
    "images, labels = next(iter(train_dataloader))\n",
    "print(f\"Train Data Image Size: {images.size()} Label Size : {labels.size()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mac gpu 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:2.0.1\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-13.4.1-arm64-arm-64bit\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다.\n",
    "!python -c 'import platform;print(platform.platform())'\n",
    "device = device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_dnn_model import FashionDNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FashionDNN model 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionDNN(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "model = FashionDNN()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FashionDNN model train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500 Loss: 0.4073633551597595 Accuracy: 83.88999938964844\n",
      "Iteration: 1000 Loss: 0.21681678295135498 Accuracy: 84.93000030517578\n",
      "Iteration: 1500 Loss: 0.3772657513618469 Accuracy: 85.37999725341797\n",
      "Iteration: 2000 Loss: 0.3036467730998993 Accuracy: 85.56999969482422\n",
      "Iteration: 2500 Loss: 0.3027021884918213 Accuracy: 86.11000061035156\n",
      "Iteration: 3000 Loss: 0.3648817837238312 Accuracy: 85.94000244140625\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images,labels in train_dataloader:\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        train = images.view(100,1,28,28)\n",
    "        labels = labels\n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if not (count % 50):\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for images,labels in test_dataloader:\n",
    "                images,labels = images.to(device),labels.to(device)\n",
    "                labels_list.append(labels)\n",
    "                test = images.view(100,1,28,28)\n",
    "                outputs = model(test)\n",
    "                predictions = torch.max(outputs,1)[1].to(device)\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()\n",
    "                total += len(labels)\n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if not (count % 500):\n",
    "            print(f\"Iteration: {count} Loss: {loss.data} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FashinCNN 모델 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=2304, out_features=600, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from fasion_cnn_model import FashionCNN\n",
    "\n",
    "learning_rate = 0.001\n",
    "model = FashionCNN()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FashionCNN 모델 train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hwanghyejeong/.local/share/virtualenvs/23-2_MMStudy_Analysis_F-O1JOBGQa/lib/python3.11/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500 Loss: 0.35280171036720276 Accuracy: 88.93000030517578\n",
      "Iteration: 1000 Loss: 0.3708013594150543 Accuracy: 88.72000122070312\n",
      "Iteration: 1500 Loss: 0.23715616762638092 Accuracy: 89.79000091552734\n",
      "Iteration: 2000 Loss: 0.22042594850063324 Accuracy: 89.44999694824219\n",
      "Iteration: 2500 Loss: 0.18529856204986572 Accuracy: 91.12999725341797\n",
      "Iteration: 3000 Loss: 0.16872002184391022 Accuracy: 89.62000274658203\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images,labels in train_dataloader:\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        train = images.view(100,1,28,28)\n",
    "        labels = labels\n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if not (count % 50):\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for images,labels in test_dataloader:\n",
    "                images,labels = images.to(device),labels.to(device)\n",
    "                labels_list.append(labels)\n",
    "                test = images.view(100,1,28,28)\n",
    "                outputs = model(test)\n",
    "                predictions = torch.max(outputs,1)[1].to(device)\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()\n",
    "                total += len(labels)\n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if not (count % 500):\n",
    "            print(f\"Iteration: {count} Loss: {loss.data} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.3. 전이 학습**\n",
    "- 전이학습\n",
    "    - 아주 큰 데이터셋을 써서 훈련된 모델의 가중치를 가져와 우리가 해결하려는 과제에 맞게 보정해서 사용하는 것\n",
    "    - 방법\n",
    "        - 특성 추출\n",
    "        - 미세 조정\n",
    "##### 5.3.1 특성 추출\n",
    "- 특성 추출\n",
    "    - 사전 훈련된 모델에서 완전 연결층(fc layer)만 다시 만든다.\n",
    "    - 학습할 때는 마지막 완전 연결층만 학습하고 나머지 계층들은 학습되지 않도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 데이터 전처리 방법 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize([256,256]),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train  데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/hwanghyejeong/Downloads/training_set\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(data_path,transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습데이터 개수와 이미지 크기 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 8005 images\n",
      "Train Data Image Size: torch.Size([32, 3, 224, 224]) Label Size : torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from data import load_data_using_mini_batch \n",
    "\n",
    "# count images in train data set\n",
    "print(f\"Train Data: {len(train_dataset)} images\")\n",
    "\n",
    "# put dataset in torch data loader\n",
    "mini_batch_size = 32\n",
    "train_dataloader = load_data_using_mini_batch(train_dataset,mini_batch_size)\n",
    "test_dataloader = load_data_using_mini_batch(test_dataset,mini_batch_size)\n",
    "\n",
    "# check image size\n",
    "images, labels = next(iter(train_dataloader))\n",
    "print(f\"Train Data Image Size: {images.size()} Label Size : {labels.size()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "23-2_MMStudy_Analysis_F-O1JOBGQa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
