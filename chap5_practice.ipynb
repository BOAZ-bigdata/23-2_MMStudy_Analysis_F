{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. 합성곱 신경망**\n",
    "#### **목차**\n",
    "- 5.1. 합성곱 신경망\n",
    "- 5.2. 합성곱 신경망 맛보기\n",
    "- 5.3. 전이 학습\n",
    "- 5.4. 설명 가능한 CNN\n",
    "- 5.5. 그래프 합성곱 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.1. 합성곱 신경망**\n",
    "##### 5.1.1 합성곱층의 필요성\n",
    "- 이미지 데이터의 경우 데이터의 공간적 구조를 무시하지 않기 위해 등장\n",
    "##### 5.1.2 합성곱 신경망 구조\n",
    "- 합성곱 신경망의 계층\n",
    "    - 입력층\n",
    "    - 합성곱층\n",
    "    - 풀링층\n",
    "        - 특성 맵의 차원을 다운 샘플링하여 연산량을 감소\n",
    "        - 종류\n",
    "            - 최대 풀링\n",
    "            - 평균 풀링\n",
    "    - 완전연결층\n",
    "    - 출력층\n",
    "##### 5.1.3 1D,2D,3D 합성곱\n",
    "- 1D 합성곱\n",
    "    - 필터가 시간을 축으로 좌우로만 이동할 수 있는 합성곱\n",
    "    - 출력 형태: 1D 배열\n",
    "- 3D 입력을 갖는 2D 합성곱\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2. 합성곱 신경망 맛보기**\n",
    "- fashion_mnist 데이터셋을 사용하여 합성곱 신경망을 직접 구현해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../chap05/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../chap05/data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\"../chap05/data\",download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\"../chap05/data\",download=True,train=False,transform=transforms.Compose([transforms.ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 데이터 사이즈와 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 60000 images\n",
      "Train Data Image Size: torch.Size([100, 1, 28, 28]) Label Size : torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "from data import load_data_using_mini_batch \n",
    "\n",
    "# count images in train data set\n",
    "print(f\"Train Data: {len(train_dataset)} images\")\n",
    "\n",
    "# put dataset in torch data loader\n",
    "mini_batch_size = 100\n",
    "train_dataloader = load_data_using_mini_batch(train_dataset,mini_batch_size)\n",
    "test_dataloader = load_data_using_mini_batch(test_dataset,mini_batch_size)\n",
    "\n",
    "# check image size\n",
    "images, labels = next(iter(train_dataloader))\n",
    "print(f\"Train Data Image Size: {images.size()} Label Size : {labels.size()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mac gpu 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:2.0.1\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-13.4.1-arm64-arm-64bit\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다.\n",
    "!python -c 'import platform;print(platform.platform())'\n",
    "device = device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_dnn_model import FashionDNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionDNN(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "model = FashionDNN()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500 Loss: 0.4073633551597595 Accuracy: 83.88999938964844\n",
      "Iteration: 1000 Loss: 0.21681678295135498 Accuracy: 84.93000030517578\n",
      "Iteration: 1500 Loss: 0.3772657513618469 Accuracy: 85.37999725341797\n",
      "Iteration: 2000 Loss: 0.3036467730998993 Accuracy: 85.56999969482422\n",
      "Iteration: 2500 Loss: 0.3027021884918213 Accuracy: 86.11000061035156\n",
      "Iteration: 3000 Loss: 0.3648817837238312 Accuracy: 85.94000244140625\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images,labels in train_dataloader:\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        train = images.view(100,1,28,28)\n",
    "        labels = labels\n",
    "        outputs = model(train)\n",
    "        loss = criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "\n",
    "        if not (count % 50):\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for images,labels in test_dataloader:\n",
    "                images,labels = images.to(device),labels.to(device)\n",
    "                labels_list.append(labels)\n",
    "                test = images.view(100,1,28,28)\n",
    "                outputs = model(test)\n",
    "                predictions = torch.max(outputs,1)[1].to(device)\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()\n",
    "                total += len(labels)\n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if not (count % 500):\n",
    "            print(f\"Iteration: {count} Loss: {loss.data} Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "23-2_MMStudy_Analysis_F-O1JOBGQa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
